{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n예전 tensorflow 하위 keras 모듈들\\nfrom tensorflow.keras.layers import Dropout\\nfrom tensorflow.keras.layers import Dense\\nfrom tensorflow.keras.layers import Embedding\\nfrom tensorflow.keras.layers import LSTM\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#문자열 전처리를 위한 모듈\n",
    "#maketran\n",
    "import string\n",
    "\n",
    "#이미지벡터추출에서 쓰이는 모듈들\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input as preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "from PIL import Image\n",
    "from keras.preprocessing import image as image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#이미지벡터 데이터셋 파일로 저장하고 다시 불러올때, 쓰이는 모듈\n",
    "import pickle\n",
    "from pickle import load\n",
    "\n",
    "#제너레이터 함수에서 쓰이는 모듈들\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "#glove 임베딩에서 쓰이는 모듈, 중복되서 주석화\n",
    "#import numpy as np\n",
    "\n",
    "#모델 정의에서 쓰이는 모듈들\n",
    "from keras import Input, layers\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
    "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import add\n",
    "\n",
    "'''\n",
    "예전 tensorflow 하위 keras 모듈들\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "from pickle import dump, load\n",
    "from time import time\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
    "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.merge import add\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras import Input, layers\n",
    "from keras import optimizers\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .\n",
      "1000268201_693b08cb0e.jpg,A girl going into a wooden building .\n",
      "1000268201_693b08cb0e.jpg,A little girl climbing into a wooden playhouse .\n",
      "1000268201_693b08cb0e.jpg,A little girl climbing the stairs to\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "train은 이미 만들어서 잠시 주석처리\n",
    "'''\n",
    "\n",
    "#filename=\"C:/MyAI/archive/train_captions.txt\"\n",
    "filename=\"C:/MyAI/archive/train_captions.txt\"\n",
    "\n",
    "\n",
    "file = open(filename, 'r')\n",
    "doc=file.read()\n",
    "\n",
    "print(doc[0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3508882611_3947c0dbf5\n",
      "['startseq A lady is running her dog through an agility course . endseq', 'startseq A large grey dog is jumping over a white hurdle . endseq', 'startseq A woman and a dog work through an agility course . endseq', 'startseq a woman running around a course with a grey dog jumping a railing endseq', 'startseq Trainer runs her dog through training course with green pipe endseq']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "아래 실행시키거나 로드\n",
    "'''\n",
    "\n",
    "#6000개 train 데이터셋 딕셔너리화\n",
    "train_descriptions = dict()\n",
    "#key 값이 될 image id를 모아둘 dataset list\n",
    "train_dataset=list()\n",
    "\n",
    "i=0\n",
    "for line in doc.split('\\n'):\n",
    "    tokens = line.split()\n",
    "    if len(line)<2:\n",
    "        continue\n",
    "\n",
    "    #image_id 와 image_desc 분리\n",
    "    tokens = line.split('.jpg,')\n",
    "    \n",
    "    #image_id,image_desc 만들기\n",
    "    image_id, image_desc = tokens[0], tokens[1]\n",
    "\n",
    "    if image_id not in train_descriptions:\n",
    "        train_dataset.append(image_id)\n",
    "        train_descriptions[image_id]=list()\n",
    "        \n",
    "        \n",
    "    desc = 'startseq ' + ''.join(image_desc) + ' endseq'\n",
    "  \n",
    "    \n",
    "    train_descriptions[image_id].append(desc)\n",
    "\n",
    "    \n",
    "#마지막 6000번째 이미지 이름에 대한 설명이 딕셔너리에 들어간지 확인.\n",
    "print(train_dataset[-1])\n",
    "print(train_descriptions[train_dataset[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1000268201_693b08cb0e',\n",
       " '1001773457_577c3a7d70',\n",
       " '1002674143_1b742ab4b8',\n",
       " '1003163366_44323f5815',\n",
       " '1007129816_e794419615']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_descriptions.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['startseq A child in a pink dress is climbing up a set of stairs in an entry way . endseq',\n",
       " 'startseq A girl going into a wooden building . endseq',\n",
       " 'startseq A little girl climbing into a wooden playhouse . endseq',\n",
       " 'startseq A little girl climbing the stairs to her playhouse . endseq',\n",
       " 'startseq A little girl in a pink dress going into a wooden cabin . endseq']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_descriptions['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "C:/MyAI/archive/Images/1000268201_693b08cb0e.jpg\n",
      "6000\n",
      "['C:/MyAI/archive/Images/1000268201_693b08cb0e.jpg', 'C:/MyAI/archive/Images/1001773457_577c3a7d70.jpg', 'C:/MyAI/archive/Images/1002674143_1b742ab4b8.jpg', 'C:/MyAI/archive/Images/1003163366_44323f5815.jpg', 'C:/MyAI/archive/Images/1007129816_e794419615.jpg']\n"
     ]
    }
   ],
   "source": [
    "#일단 6000개만 별도로 뺀걸 재확인\n",
    "print(len(train_dataset))\n",
    "\n",
    "#나중에 원하는 이미지만 찾아가기위한 path lisd 생성\n",
    "train_image_path_list=list()\n",
    "image_path=''\n",
    "\n",
    "#문자열 합치는 과정을 위한 예시\n",
    "image_path='C:/MyAI/archive/Images/' + train_dataset[i]+'.jpg'\n",
    "print(image_path)\n",
    "\n",
    "#train set image path list 생성하기\n",
    "for i in range(len(train_dataset)):\n",
    "    train_image_path_list.append('C:/MyAI/archive/Images/' + train_dataset[i]+'.jpg')\n",
    "\n",
    "#test\n",
    "print(len(train_image_path_list))\n",
    "\n",
    "print(train_image_path_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n",
       " 'startseq girl going into wooden building endseq',\n",
       " 'startseq little girl climbing into wooden playhouse endseq',\n",
       " 'startseq little girl climbing the stairs to her playhouse endseq',\n",
       " 'startseq little girl in pink dress going into wooden cabin endseq']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "여기는 하다보니 실행시켰는데 이 아래는 caption 데이터 전처리라 무의미\n",
    "'''\n",
    "\n",
    "# table은 딕셔너리형태이고, 첫번째 아규먼트는 key가 될 문자열, 두번째는 value가 될 문자열, 두개는 길이가 같아야한다.\n",
    "# 세번째 아규먼트는 해당 value를 제외하고 싶은 key값import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "for key, desc_list in train_descriptions.items():\n",
    "    for i in range(len(desc_list)):\n",
    "        desc = desc_list[i]\n",
    "        # 리스트화(tokenize), 결과적으로 1문장이 스페이스 기준으로 한글자씩 리스트화\n",
    "        desc = desc.split()\n",
    "        # 소문자화\n",
    "        desc = [word.lower() for word in desc]\n",
    "        # 구두점 제거\n",
    "        desc = [w.translate(table) for w in desc]\n",
    "        # remove hanging 's' and 'a'\n",
    "        desc = [word for word in desc if len(word)>1]\n",
    "        # remove tokens with numbers in them\n",
    "        desc = [word for word in desc if word.isalpha()]\n",
    "        # store as string\n",
    "        desc_list[i] =  ' '.join(desc)\n",
    "        \n",
    "#caption에 구두점 \"\"가 있는 이미지 997338199_7343367d7f 확인\n",
    "train_descriptions['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Vocabulary Size: 7667\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "for key in train_descriptions.keys():\n",
    "    [vocabulary.update(d.split()) for d in train_descriptions[key]]\n",
    "print('Original Vocabulary Size: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일에 description 저장\n",
    "def save_descriptions(descriptions, filename):\n",
    "        lines = list()\n",
    "        for key, desc_list in descriptions.items():\n",
    "            for desc in desc_list:\n",
    "                lines.append(key + ' ' + desc)\n",
    "        data = '\\n'.join(lines)\n",
    "        file = open(filename, 'w')\n",
    "        file.write(data)\n",
    "        file.close()\n",
    "\n",
    "save_descriptions(train_descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train caption 파일 불러오기\n",
    "# 예제 코드 -> 대부분 함수로 선언해서 값 return 받는 형태\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n"
     ]
    }
   ],
   "source": [
    "# photo identifiers 리스트 호출\n",
    "def load_set(filename):\n",
    "        doc = load_doc(filename)\n",
    "        dataset = list()\n",
    "        for line in doc.split('\\n'):\n",
    "            # 비어있는 라인 skip\n",
    "            if len(line) < 1:\n",
    "                continue\n",
    "\n",
    "            identifier = line.split('.')[0]\n",
    "            dataset.append(identifier)\n",
    "        return set(dataset)\n",
    "\n",
    "# train 데이터 불러오기\n",
    "filename = \"C:/MyAI/archive/train_captions.txt\"\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed words 1667 \n"
     ]
    }
   ],
   "source": [
    "# 빈출 어휘만 추출\n",
    "\n",
    "all_train_captions = []\n",
    "for key, val in train_descriptions.items():\n",
    "    for cap in val:\n",
    "        all_train_captions.append(cap)\n",
    "\n",
    "# 10번 이상 언급된애만\n",
    "word_count_threshold = 10\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for sent in all_train_captions:\n",
    "    nsents += 1\n",
    "    for w in sent.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "\n",
    "print('preprocessed words %d ' % len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_descriptions : 6000\n",
      "train_dataset: 6000\n",
      "1000268201_693b08cb0e\n",
      "train_image_path_list: 6000\n",
      "C:/MyAI/archive/Images/1000268201_693b08cb0e.jpg\n",
      "Original Vocabulary Size: 7667\n",
      "preprocessed words 1667 \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "중간 변수들 종합\n",
    "0. train_descriptions ; image id 가 key 이고 caption들이 value 인 딕셔너리\n",
    "1. train_dataset ; train_descriptions에서 key 값들이 모여있는 리스트\n",
    "2. train_image_path_list ; 최초 다운받은 폴더의 Images 폴더에서 선별된 6000개 또는 2091개의 image만 찾아가기 위한 경로 모음(리스트)\n",
    "3. vocabulary와 voca ; 전체 어휘와 빈출어휘\n",
    "4. max_length ; 캡션 최대 길이\n",
    "'''\n",
    "print('train_descriptions : %d'% len(train_descriptions))\n",
    "print('train_dataset: %d'% len(train_dataset))\n",
    "print(train_dataset[0])\n",
    "print('train_image_path_list: %d'% len(train_image_path_list))\n",
    "print(train_image_path_list[0])\n",
    "print('Original Vocabulary Size: %d' % len(vocabulary))\n",
    "print('preprocessed words %d ' % len(vocab))\n",
    "#print('Max CNT of Word of caption(description): %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train caption set을 인덱싱화하자 generator function에 넣기 위해.\n",
    "ixtoword = {}\n",
    "wordtoix = {}\n",
    "ix = 1\n",
    "for w in vocab:\n",
    "    wordtoix[w] = ix\n",
    "    ixtoword[ix] = w\n",
    "    ix += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1668\n"
     ]
    }
   ],
   "source": [
    "vocab_size= len(ixtoword)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max CNT of Word of caption(description): 34\n"
     ]
    }
   ],
   "source": [
    "#필요변수인 Max Description length를 구하자 generator function에 넣기 위해.\n",
    "# dictionary 에서 list로\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "    \n",
    "# clean된 caption들 중 가장 긴 문자은 단어가 몇개 일까?\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)\n",
    "\n",
    "# 제일 단어가 많이 쓰인 catpion에서 단어의 갯수\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Max CNT of Word of caption(description): %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n관련 링크\\nhttps://keras.io/api/applications/inceptionv3/\\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3\\n\\nModel 관련\\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model\\nhttps://ballentain.tistory.com/26\\n차원하는 이유 관련\\nhttps://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/inception.py#L64\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#8. Data Preprocessing — Images\n",
    "'''\n",
    "pkl파일로 이미 전처리 결과를 파일로 만들었으므로, 그냥 파일만 업로드하면됨.\n",
    "\n",
    "\n",
    "관련 링크\n",
    "https://keras.io/api/applications/inceptionv3/\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3\n",
    "\n",
    "Model 관련\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "https://ballentain.tistory.com/26\n",
    "차원하는 이유 관련\n",
    "https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/inception.py#L64\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=299x299 at 0x1A4F394C588>\n",
      "(299, 299, 3)\n",
      "(1, 299, 299, 3)\n",
      "After preprocess_input  (1, 299, 299, 3)\n",
      "After model_new  (1, 2048)\n",
      "(2048,)\n",
      "[0.5831281  0.19629233 0.36157233 ... 0.331434   0.98387724 0.29585987]\n"
     ]
    }
   ],
   "source": [
    "model = InceptionV3(weights='imagenet')\n",
    "model_new = Model(model.input, model.layers[-2].output)\n",
    "\n",
    "# example\n",
    "image_path='C:/MyAI/archive/Images/3510695264_ef460fa6cc.jpg'\n",
    "img = image.load_img(image_path, target_size=(299, 299))\n",
    "\n",
    "print(img)\n",
    "\n",
    "# PIL 이미지에 차원 추가 for inception_v3\n",
    "x = image.img_to_array(img)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "# PIL 이미지에 차원 추가 for inception_v3\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "# inception_v3에서 제공하는 preprocess_input()\n",
    "x = preprocess_input(x)\n",
    "print(\"After preprocess_input \",x.shape)\n",
    "\n",
    "# model_new 활용\n",
    "x=model_new(x)\n",
    "print(\"After model_new \",x.shape)\n",
    "\n",
    "# reshape from (1, 2048) to (2048, )\n",
    "x = np.reshape(x, x.shape[1])\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2091\n",
      "2091\n",
      "[0.2336708  1.4837426  0.5093453  ... 0.61955357 0.8530511  0.6244722 ]\n",
      "(2048,)\n"
     ]
    }
   ],
   "source": [
    "#image feature vector list 생성\n",
    "image_feature_vector_list=[]\n",
    "print(len(train_image_path_list))\n",
    "\n",
    "'''\n",
    "원래라면 위에서부터 train_blabla 말고 test_blabla라고 명명했어야하는데,\n",
    "지금 짧은 시간 내에 일일이 바꾸기에는 촉박해서 그냥 변수명은 train_blabla인데,\n",
    "내용물은 test 데이터셋입니다.\n",
    "'''\n",
    "\n",
    "\n",
    "for i in range(len(train_image_path_list)):\n",
    "    train_image_path=train_image_path_list[i]\n",
    "    img = image.load_img(train_image_path, target_size=(299, 299))\n",
    "    vector_x=image.img_to_array(img)\n",
    "    vector_x = np.expand_dims(vector_x, axis=0)\n",
    "    vector_x = preprocess_input(vector_x)\n",
    "    vector_x=model_new(vector_x)\n",
    "    vector_x = np.reshape(vector_x, vector_x.shape[1])\n",
    "    image_feature_vector_list.append(vector_x)\n",
    "\n",
    "#test\n",
    "print(len(image_feature_vector_list))\n",
    "print(image_feature_vector_list[0])\n",
    "print(image_feature_vector_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3509575615_653cbf01fc\n",
      "3509575615_653cbf01fc.jpg [0.2336708  1.4837426  0.5093453  ... 0.61955357 0.8530511  0.6244722 ]\n",
      "3509611207_7645b1d28d.jpg [0.48645875 0.2508113  1.2043017  ... 0.11045711 0.77078986 1.038936  ]\n",
      "3510218982_318f738b76.jpg [0.68048835 0.62899965 0.27120814 ... 0.23872076 0.4604826  0.38286105]\n",
      "3510219078_670b6b3157.jpg [0.28584427 0.34881198 0.15260641 ... 0.13671395 0.19284974 0.20186724]\n",
      "3510695264_ef460fa6cc.jpg [0.5831281  0.19629233 0.36157233 ... 0.331434   0.98387724 0.29585987]\n",
      "3511062827_cd87871c67.jpg [0.12558027 0.22470963 0.15222874 ... 0.9616679  0.15674192 0.3367479 ]\n",
      "3511890331_6163612bb9.jpg [0.3843404  0.32294887 0.43583447 ... 0.41908535 0.54027665 0.1667529 ]\n",
      "3512033659_7e8a0c2ffa.jpg [0.10077284 0.6326918  1.0875754  ... 0.12817284 0.49915388 0.27155975]\n",
      "3512033861_a357bb58b6.jpg [0.08709924 0.8029965  0.01752065 ... 0.         0.03960565 0.40201607]\n",
      "3512127856_18a4c7aace.jpg [0.4035572  0.18981698 0.26122856 ... 0.42452398 0.55984527 0.37261876]\n"
     ]
    }
   ],
   "source": [
    "#train dataset의 원소는 3508882611_3947c0dbf5와 같이 확장자 없는 순수 이름을 형태로한 image_id\n",
    "print(train_dataset[0])\n",
    "\n",
    "'''\n",
    "다행히 image_feature_vector_list의 순서는 train_dataset을 바탕으로 함. 즉 train_dataset의 n번째 값에 대응하는 image는\n",
    "image_feature_vector_list의 n번째 값에 대응하는 image와 동일\n",
    "결론적으로 n번째 이미지의 id는 train_dataset의 n번째, 벡터값은 image_feature_vector_list의 n번째에 위치.\n",
    "\n",
    "\n",
    "다만 주어진 딕셔너리 photos의 key값은 위 train_dataset의 원소 +.jpg꼴이라 약간의 변형 필요.\n",
    "'''\n",
    "\n",
    "'''\n",
    "원래라면 위에서부터 train_blabla 말고 test_blabla라고 명명했어야하는데,\n",
    "지금 짧은 시간 내에 일일이 바꾸기에는 촉박해서 그냥 변수명은 train_blabla인데,\n",
    "내용물은 test 데이터셋입니다.\n",
    "'''\n",
    "\n",
    "\n",
    "# train_dataset의 원소 +.jpg\n",
    "key_photos=[]\n",
    "for i in range(len(train_dataset)):\n",
    "    key_photo=train_dataset[i]+'.jpg'\n",
    "    key_photos.append(key_photo)\n",
    "\n",
    "#딕셔너리 photos 만들기\n",
    "photos=dict()\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    photos[key_photos[i]]=image_feature_vector_list[i]\n",
    "\n",
    "#결과 일부 확인\n",
    "\n",
    "i=0\n",
    "for key, value in photos.items():\n",
    "    if(i==10):\n",
    "        break\n",
    "    print(key, value)\n",
    "    i=i+1\n",
    "    \n",
    "# Save the bottleneck test features to disk\n",
    "with open(\"C:/MyAI/archive/encoded_test_images.pkl\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(photos, encoded_pickle)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photos: train=6000\n",
      "1000268201_693b08cb0e.jpg [0.1227764  0.3329308  0.7527251  ... 0.2194151  0.30208534 0.4027964 ]\n",
      "1001773457_577c3a7d70.jpg [0.73668337 0.591132   0.18194185 ... 0.16030258 0.3414473  0.9083165 ]\n",
      "1002674143_1b742ab4b8.jpg [0.37349278 0.24600588 0.9633662  ... 1.1460401  0.2655153  0.01984664]\n",
      "1003163366_44323f5815.jpg [0.48604354 0.29217845 0.31434855 ... 0.9825365  0.2810954  0.22330505]\n",
      "1007129816_e794419615.jpg [0.21595949 0.15582629 0.29477617 ... 0.40562844 0.16532421 0.37413427]\n",
      "1007320043_627395c3d8.jpg [0.17230766 0.49810642 0.21577148 ... 0.09679462 1.2298679  0.5421516 ]\n",
      "1009434119_febe49276a.jpg [0.53430766 1.3675833  0.7805752  ... 0.39465535 0.8875862  1.072792  ]\n",
      "1012212859_01547e3f17.jpg [0.24028873 1.0862249  0.04511875 ... 0.00142372 0.09225179 0.79243594]\n",
      "1015118661_980735411b.jpg [0.23118024 0.12730137 0.39619404 ... 0.05207124 0.5935346  0.8125621 ]\n",
      "1015584366_dfcec3c85a.jpg [0.4532365  0.4290843  0.2021637  ... 0.3568901  0.38221988 0.15984103]\n",
      "(2048,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "이 부분이 저장한 pkl파일을 업로드하는 부분\n",
    "자료형도 (2048,) 확인\n",
    "\n",
    "'''\n",
    "\n",
    "train_features = load(open(\"C:/MyAI/archive/encoded_train_images.pkl\", \"rb\"))\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "\n",
    "i=0\n",
    "for key, value in train_features.items():\n",
    "    if(i==10):\n",
    "        break\n",
    "    print(key, value)\n",
    "    i=i+1\n",
    "    \n",
    "print(train_features['1000268201_693b08cb0e.jpg'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATOR !!!!!!!!\n",
    "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            n+=1\n",
    "            # retrieve the photo feature\n",
    "            photo = photos[key+'.jpg']\n",
    "            for desc in desc_list:\n",
    "                # encode the sequence\n",
    "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # input 시퀀스랑 output 시퀀스로 분리\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # 인풋시퀀스 제로 패딩해서 길이 max로 맞추기\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # 아웃풋 시퀀스 인코딩(원핫 인코딩 추정), 1은 [1.0, 0, 0, blabla], 2는 [0, 1.0, 0, blabla]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "   \n",
    "                    X1.append(photo)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "\n",
    "            if n==num_photos_per_batch:\n",
    "                return [[array(X1), array(X2)], array(y)]\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load Glove vectors\n",
    "glove_dir = 'C:/MyAI/archive'\n",
    "embeddings_index = {} # empty dictionary\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "#voca_size란 max_length인줄 알았는데 아니고 전체 단어 갯수인듯\n",
    "#근데 왜 선별된 1667이 아닌지는 몰겠음\n",
    "vocab_size=1667+1\n",
    "\n",
    "# Get 200-dim dense vector for each of the 10000 words in out vocabulary\n",
    "# 일단 문법그대로라면 np.zeros를 통해서 34행 200열의 영행렬이 생성\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in wordtoix.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1668, 200)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image feature extractor model\n",
    "inputs1 = Input(shape=(2048,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "# partial caption sequence model\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# decoder (feed forward) model\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "# merge the two input models\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 34)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 34, 200)      333600      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 34, 200)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          467968      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1668)         428676      dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,820,580\n",
      "Trainable params: 1,820,580\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\schiz\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    C:\\Users\\schiz\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\schiz\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\schiz\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\schiz\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:541 train_step  **\n        self.trainable_variables)\n    C:\\Users\\schiz\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1804 _minimize\n        trainable_variables))\n    C:\\Users\\schiz\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:521 _aggregate_gradients\n        filtered_grads_and_vars = _filter_grads(grads_and_vars)\n    C:\\Users\\schiz\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:1219 _filter_grads\n        ([v.name for _, v in grads_and_vars],))\n\n    ValueError: No gradients provided for any variable: ['dense/kernel:0', 'dense/bias:0', 'lstm/lstm_cell/kernel:0', 'lstm/lstm_cell/recurrent_kernel:0', 'lstm/lstm_cell/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0'].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-e90dd2f512a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_descriptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordtoix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_pics_per_bath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/MyAI/archive/model_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1477\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1478\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1479\u001b[1;33m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1481\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    625\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 506\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2445\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2446\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2447\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2777\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2779\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2667\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2669\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\schiz\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    C:\\Users\\schiz\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\schiz\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\schiz\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\schiz\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:541 train_step  **\n        self.trainable_variables)\n    C:\\Users\\schiz\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1804 _minimize\n        trainable_variables))\n    C:\\Users\\schiz\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:521 _aggregate_gradients\n        filtered_grads_and_vars = _filter_grads(grads_and_vars)\n    C:\\Users\\schiz\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:1219 _filter_grads\n        ([v.name for _, v in grads_and_vars],))\n\n    ValueError: No gradients provided for any variable: ['dense/kernel:0', 'dense/bias:0', 'lstm/lstm_cell/kernel:0', 'lstm/lstm_cell/recurrent_kernel:0', 'lstm/lstm_cell/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0'].\n"
     ]
    }
   ],
   "source": [
    "model.layers[2]\n",
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "epochs = 10\n",
    "number_pics_per_bath = 3\n",
    "steps = len(train_descriptions)//number_pics_per_bath\n",
    "\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train_descriptions, train_features, wordtoix, max_length, number_pics_per_bath)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    model.save('C:/MyAI/archive/model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    generator = data_generator(train_descriptions, train_features, wordtoix, max_length, number_pics_per_bath)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    model.save('C:/Users/User/Desktop/객체_플젝' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, RMSprop\n",
    "#lr은 학습률, 예전에 이론 때 alpha라고 배운 변수명, 0~1사이의 값, 그 손실함수에서 극소값 찾아갈때 사용되는 변수, 이동 단위의 변수랄가\n",
    "keras.optimizers.Adam(lr=0.001)\n",
    "epochs = 10\n",
    "number_pics_per_bath = 6\n",
    "steps = len(train_descriptions)//number_pics_per_bath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    generator = data_generator(train_descriptions, train_features, wordtoix, max_length, number_pics_per_bath)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    # model.save('C:/Users/User/Desktop/객체_플젝' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('C:/Users/User/Desktop/객체_플젝30.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('C:/Users/User/Desktop/객체_플젝30.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Users/User/Desktop/객체_플젝/encoded_test_images.pkl\", \"rb\") as encoded_pickle:\n",
    "    encoding_test = load(encoded_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedySearch(photo):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = ixtoword[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_list=list()\n",
    "\n",
    "pic_list.append('3510218982_318f738b76.jpg')\n",
    "pic_list.append('3513362553_5fc5779e20.jpg')\n",
    "pic_list.append('3516267455_ca17cc1323.jpg')\n",
    "pic_list.append('3518675890_2f65e23ff9.jpg')\n",
    "pic_list.append('3520321387_710ab74cda.jpg')\n",
    "\n",
    "pic_list.append('3525417522_7beb617f8b.jpg')\n",
    "pic_list.append('3535284878_f90f10236e.jpg')\n",
    "pic_list.append('3537520829_aab733e16c.jpg')\n",
    "pic_list.append('3538686658_30afc75f02.jpg')\n",
    "pic_list.append('3540598210_972f0ff573.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in range(0,10,1):\n",
    "    #pic is image_id like '3516267455_ca17cc1323.jpg'\n",
    "    #pic = list(encoding_test.keys())[z]\n",
    "    pic=pic_list[z]\n",
    "    #image is reshaped vector of image\n",
    "    image = encoding_test[pic].reshape((1,2048))\n",
    "    #images+pic is path of image that has image_id saved at pic\n",
    "    x=plt.imread(images+pic)\n",
    "    plt.imshow(x)\n",
    "    plt.show()\n",
    "    print(\"Greedy:\",greedySearch(image))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
